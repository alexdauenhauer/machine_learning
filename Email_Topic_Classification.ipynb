{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train various classifiers to distinguish between topics based on their text. Each document will be represented with by a \"bag-of-words\" model resulting in sparse feature vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.2\n",
      "1.15.3\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "print sklearn.__version__\n",
    "print np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and strip out the metadata so that the calssifiers only use textual features. By default, newsgroups data is split into train and test sets. I will further split the test so I have a dev set. For simplicity, only using four categories from the set for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=labels)\n",
    "newsgroups_test = fetch_20newsgroups(subset='test',\n",
    "                                     remove=('headers', 'footers', 'quotes'),\n",
    "                                     categories=labels)\n",
    "\n",
    "num_test = len(newsgroups_test.target)\n",
    "test_data, test_labels = newsgroups_test.data[num_test/2:], newsgroups_test.target[num_test/2:]\n",
    "dev_data, dev_labels = newsgroups_test.data[:num_test/2], newsgroups_test.target[:num_test/2]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print 'training label shape:', train_labels.shape\n",
    "print 'test label shape:', test_labels.shape\n",
    "print 'dev label shape:', dev_labels.shape\n",
    "print 'labels names:', newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print out the message and label for each of the first 5 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples=5\n",
    "names = newsgroups_train['target_names']\n",
    "for n in range(num_examples):\n",
    "    print '\\nSample number:', n\n",
    "    print 'Classification Label:', names[train_labels[n]]\n",
    "    print '\\nText:\\n', train_data[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the text features using CountVectorizer()\n",
    "Getting to know the CountVectorizor() function through a number of steps. Fit and transform the data, then look at:  \n",
    "> (a) The size of the vocabulary, the average number of non-zero features per example, the fraction of the entries in the matrix that are non-zero  \n",
    "(b) The first and last feature strings of the vocabulary in alphabetical order  \n",
    "(c) Find the average number of non-zero features per example using a vocabulary of just the main words in each label [\"atheism\", \"graphics\", \"space\", \"religion\"]  \n",
    "(d) Look at the size of the vocabulary using bigram and trigam cahracter features instead of unigram word features  \n",
    "(e) Look at the size of the vocabulary after pruning words that appear in less than 10 documents  \n",
    "(f) with default settings to CountVectorizer, what fraction of words in the dev data are missing from the training data vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## PART A\n",
    "print 'PART (A):'\n",
    "\n",
    "# vectorize the features, fit_transform them into a sparse matrix\n",
    "v = CountVectorizer()\n",
    "features = v.fit_transform(train_data)\n",
    "print '\\nSize of vocabulary:', len(v.vocabulary_)\n",
    "\n",
    "# find the mean number of nonzero features per example\n",
    "avg_nz = 1.0 * np.sum([f.nnz for f in features]) / features.shape[0]\n",
    "print 'Average number of non-zero features per example:', avg_nz\n",
    "\n",
    "# find the fraction of nonzero entries in the features matrix\n",
    "frac_nz = 1.0 * features.nnz / features.toarray().size\n",
    "print 'Fraction of entries that are non-zero:', frac_nz\n",
    "\n",
    "## PART B\n",
    "print '\\nPART (B):'\n",
    "\n",
    "# get the first and last feature strings\n",
    "names = v.get_feature_names()\n",
    "print '\\n0th feature: {}, last feature: {}'.format(names[0], names[-1])\n",
    "\n",
    "## PART C\n",
    "print '\\nPART (C):'\n",
    "# specify the vocab and fit_transform the training data\n",
    "v2 = CountVectorizer(vocabulary=['atheism', 'graphics', 'space', 'religion'])\n",
    "features2 = v2.fit_transform(train_data)\n",
    "\n",
    "# find the mean number of nonzero features per document\n",
    "avg_nz = 1.0 * np.sum([f.nnz for f in features2]) / features2.shape[0]\n",
    "print '\\nVector Shape:', features2.shape\n",
    "print 'Average number of non-zero features per example:', avg_nz\n",
    "\n",
    "## PART D\n",
    "print '\\nPART (D):'\n",
    "\n",
    "# extract bigram and trigram character features\n",
    "v3 = CountVectorizer(analyzer='char', ngram_range=(2, 3))\n",
    "v3.fit_transform(train_data)\n",
    "\n",
    "# get the size of the vocabulary\n",
    "n = len(v3.vocabulary_)\n",
    "print '\\nSize of vocabulary using bigrams and trigrams:', n\n",
    "\n",
    "## PART E\n",
    "print '\\nPART (E):'\n",
    "\n",
    "# change min_df so doc frequency is >= 10 total documents\n",
    "v4 = CountVectorizer(min_df=1.0 * 10/len(train_data))\n",
    "v4.fit_transform(train_data)\n",
    "print '\\nSize of vocabulary appearing in >= 10 docs:', len(v4.vocabulary_)\n",
    "\n",
    "## PART F\n",
    "print '\\nPART (F):'\n",
    "\n",
    "# get the vocab of the training data and the dev data\n",
    "v = CountVectorizer()\n",
    "v.fit_transform(train_data)\n",
    "train_vocab = v.get_feature_names()\n",
    "v.fit_transform(dev_data)\n",
    "dev_vocab = v.get_feature_names()\n",
    "\n",
    "# find the fraction of words in dev data vocab that are not in the \n",
    "# training data vocab\n",
    "diff = 1.0 * len(set(dev_vocab).difference(set(train_vocab))) / len(train_vocab)\n",
    "print '\\nFraction of words in dev_data and not in train_data vocab:', diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare simplistic versions of three classification models\n",
    "Find the optimal value for `k` in using a kNN model, then find the optimal value for `alpha` using a Multinomial Naive Bayes model, then find the optimal value for the regularization stregnth `C` in a Logistic Regression model. Look at the relationship between the weight vector for each class vs. the sum squared weight values for each setting of the regularization strength `C`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_plotter(start, f, train_labels, labels, c_vals):\n",
    "    ssq_total = []\n",
    "#     for c in np.arange(1,100,2):\n",
    "    for c in c_vals:\n",
    "\n",
    "        # fit a logistic regression model with regularization strength c\n",
    "        lg = LogisticRegression(C=c)\n",
    "        lg.fit(f, train_labels)\n",
    "\n",
    "        # Take the sum of the squared weights for each label for each \n",
    "        # value of c and append it to a list\n",
    "        weights = lg.coef_\n",
    "        ssq = np.sum(weights**2, axis=1)\n",
    "        ssq_total.append(ssq)\n",
    "    # convert to a numpy array\n",
    "    ssq_total = np.array(ssq_total)\n",
    "    print 'runtime:', time.time() - start\n",
    "\n",
    "    # plot the regularization strength vs. the sum of the squared\n",
    "    # weights for each label\n",
    "    plt.figure(figsize=(12,8))\n",
    "    for i in range(4):\n",
    "        plt.plot(c_vals, ssq_total[:,i])\n",
    "    t = (\n",
    "        'Regularization Strength vs. Sum Squared Feature Weights\\n' + \n",
    "        'C value range: min = {},'.format(c_vals[0]) + \n",
    "        ' max = {},'.format(c_vals[-1]) + \n",
    "        ' step size = {}'.format(c_vals[1] - c_vals[0])\n",
    "    )\n",
    "    plt.title(t)\n",
    "    plt.xlabel('Regularization Strength (C)')\n",
    "    plt.ylabel('Sum Squared Feature Weights')\n",
    "    plt.legend([labels[0], labels[1], labels[2], labels[3]])\n",
    "\n",
    "    \n",
    "v = CountVectorizer()\n",
    "f = v.fit_transform(train_data)\n",
    "scorer = metrics.make_scorer(metrics.f1_score, average='weighted')\n",
    "\n",
    "## k-NN classifier\n",
    "# Use kNN classifier and optimize the number of neighbors using grid \n",
    "# search with the f1-score as the scoring metric\n",
    "print '\\nK-NEAREST NEIGHBORS MODEL:\\n'\n",
    "start = time.time()\n",
    "\n",
    "# n_neighbors was tested over a much larger range, but the optimal \n",
    "# value was found in the range below, so I shortened the search \n",
    "# range to improve runtime\n",
    "params = {'n_neighbors': np.arange(90, 100)}\n",
    "gs = GridSearchCV(KNeighborsClassifier(), params, scoring=scorer)\n",
    "gs.fit(f, train_labels)\n",
    "end = time.time()\n",
    "print 'Best k:', gs.best_params_.values()[0]\n",
    "print 'Best F1-score:', gs.best_score_ \n",
    "print 'runtime:', round(end-start, 3)\n",
    "\n",
    "## Multinomial NB\n",
    "# Use the multinomial Naive Bayes classifier and optimize the alpha \n",
    "# value using grid search with the f1-score as the scoring metric\n",
    "print '\\nMULTINOMIAL NAIVE BAYES MODEL:\\n'\n",
    "start = time.time()\n",
    "\n",
    "# alpha was tested over a much larger range, but the optimal \n",
    "# value was found in the range below, so I shortened the search \n",
    "# range to improve runtime\n",
    "params = {'alpha': np.arange(0.0, 0.01, 0.0001)}\n",
    "gs = GridSearchCV(MultinomialNB(), params, scoring=scorer)\n",
    "gs.fit(f, train_labels)\n",
    "end = time.time()\n",
    "print 'Best alpha:', gs.best_params_.values()\n",
    "print 'Best F1-score:', gs.best_score_ \n",
    "print 'runtime:', round(end-start, 3)\n",
    "\n",
    "## Logistic Regression\n",
    "# Use logistic regression and optimize the regularization strength\n",
    "# using grid search with the f1-score as the scoring metric\n",
    "print '\\nLOGISTIC REGRESSION MODEL:\\n'\n",
    "start = time.time()\n",
    "\n",
    "# regularization strength was tested over a much larger range, but \n",
    "# the optimal value was found in the range below, so I shortened the\n",
    "# search range to improve runtime\n",
    "params = {'C': np.arange(0.1, .2, 0.01)}\n",
    "gs = GridSearchCV(LogisticRegression(), params, scoring=scorer)\n",
    "gs.fit(f, train_labels)\n",
    "end = time.time()\n",
    "print 'Best regularization parameter:', gs.best_params_.values()\n",
    "print 'Best F1-score:', gs.best_score_ \n",
    "print 'runtime:', round(end-start, 3)\n",
    "\n",
    "## Part C: \n",
    "# describe the relationship between accuracy and the sum of the \n",
    "# squared weights\n",
    "labels = newsgroups_train['target_names']\n",
    "start = time.time()\n",
    "c_plotter(start, f, train_labels, labels, c_vals=np.arange(0.001,1,0.01))\n",
    "start = time.time()\n",
    "c_plotter(start, f, train_labels, labels, c_vals=np.arange(1, 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nearest neighbors model doesn't work very well in this scenario because there aren't any neighbors that are very near. By this I mean that the vectorized features are a sparse matrix so a large majority of the data that is used as nearest neighbors will be empty or at least not contain meaningful information. This is why the optimal number of neighbors was so high at 96, because we need to include that many points in the sparse matrix to have a few points of meaningful information\n",
    "    \n",
    "As discussed here https://medium.com/@sangha_deb/naive-bayes-vs-logistic-regression-a319b07a5d4c, The reason logistic regression does not work as well as Naive Bayes for this data set is likely due to a relatively small sample size. Both Naive Bayes and Logistic Regression will reach an asymptotic solution of predictive accuracy as sample size increases, but Naive Bayes will reach that asymptotic solution faster due to operating at O(log(n)) as compared to Logistic Regression with O(n). So the training set is 2034 samples and this is apparently not enough for Logistic Regression to win out. If we had 10,000 or 100,000 samples, Logistic Regression would likely perform better.\n",
    "    \n",
    "The relationship between the regularization strength, C, and the sum of squared weights is somewhat logarithmic. This makes sense because the regularization strength is a penalty to the total size of the weight to ensure that the sum squared weight does not become too large and thus overfit the data. So if the regularization is growing linearly, then the sum squared weights must grow less than linearly, but will still be growing, and this shows in the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing weights of top 5 words for each label\n",
    "Determine the five words with the largest weights for each of the four labels for 20 words in total. Create a table showing the weight of each of these freatures for each of the four labels. Create the same table using bigram features only, then repeat using unigram and bigram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The below function generates a word map which maps the top 5 words to \n",
    "their index in the logistic regression coefficient matrix. Then using \n",
    "those indices, find the coefficient/weight of each word for each \n",
    "label. The function returns a pandas DataFrame with the rownames as \n",
    "the array of top 5 words for each label, the columns names as each \n",
    "label, and the entries as the weight of that word for each label\n",
    "'''\n",
    "def top5_table(train_data, train_labels, grams=(1,1), num_words=5):\n",
    "    \n",
    "    # get the target labels\n",
    "    labels = newsgroups_train['target_names']\n",
    "    \n",
    "    # vectorize the data and extract the vocabulary\n",
    "    v = CountVectorizer(ngram_range=grams)\n",
    "    f = v.fit_transform(train_data)\n",
    "    vocab = v.vocabulary_\n",
    "    \n",
    "    # fit the logisitic regression to extract the coefficients (weights) \n",
    "    # of the feature vectors\n",
    "    lg = LogisticRegression(C=0.18)\n",
    "    lg.fit(f, train_labels)\n",
    "    \n",
    "    # generate the weight map\n",
    "    word_map = {}\n",
    "    words = []\n",
    "    for ind,c in enumerate(lg.coef_):\n",
    "        \n",
    "        # find the top 5 words by weight for each label.\n",
    "        top5 = np.argsort(c)[::-1][:num_words]\n",
    "        words = [(k, v) for k,v in vocab.items() if v in top5]\n",
    "        \n",
    "        # store top 5 for each label in single word map\n",
    "        for w in words:\n",
    "            word_map[w[0]] = w[1]\n",
    "    \n",
    "    # grab the weights at the indices in the word map\n",
    "    weight_arr = np.array([\n",
    "        [lg.coef_[i][v] for v in word_map.values()] \n",
    "        for i in range(len(labels))\n",
    "    ])\n",
    "    \n",
    "    # generate the DataFrame\n",
    "    df = pd.DataFrame(weight_arr.T, columns=labels, index=word_map.keys())\n",
    "    \n",
    "    return df\n",
    "\n",
    "### STUDENT END ###\n",
    "df1 = top5_table(train_data, train_labels)\n",
    "df2 = top5_table(train_data, train_labels, grams=(2,2))\n",
    "df3 = top5_table(train_data, train_labels, grams=(1,2))\n",
    "print '\\nUnigram features only:\\n'\n",
    "print df1\n",
    "print '\\nBigram features only:\\n'\n",
    "print df2\n",
    "print '\\nUnigram and Bigram features:\\n'\n",
    "print df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each label, the 5 words that have the strongest weight for that label carry very strong positive coefficients which is to be expected. What is interesting is that the other 15 words that best represent the other 3 labels carry strong negative coefficients for that label, which indicates that the regression has identified the strong indicators for each label and weighted them negatively for every other label which helps to reinforce the classification. Additionally, the use of bigrams only chooses some interesting selections as the most important words for each label. With unigrams only, the words that were selected made good logical sense that they would carry strong weight for that label (e.g. 'nasa' for sci.space, 'christian' for talk.religion.misc, and 'atheism' for alt.atheism). The bigram features are harder to logically justify. Why is 'cheers kent' the strongest indicator of the label alt.atheism? How is 'and such' a strong indicator of sci.space? In running the regression with both unigrams and bigrams, it becomes apparent that these aren't actually that strong of indicators as the 20 word list in this regression contains nearly the same words as it does when using just unigrams, indicating that the bigram features do not influence the classification as strongly as the unigram features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the regression classifier\n",
    "pass a custom preprocessor to CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empty_preprocessor(s):\n",
    "    return s\n",
    "\n",
    "def better_preprocessor(s):\n",
    "    # make everything lowercase\n",
    "    s = s.lower()\n",
    "    \n",
    "    # replace standalone numeric strings with the token \"numeric_string\"\n",
    "    s = re.sub(r\"\\b\\d+\\b\", \"numeric_string\", s)\n",
    "    \n",
    "    # remove all apostrophes\n",
    "    s = re.sub(r\"\\'\", \"\", s)\n",
    "    \n",
    "    # replace all email addresses with token \"email_string\"\n",
    "    s = re.sub(r\"\\b\\w+@\\w+[\\.\\w+]+\\b\", \"email_string\", s)\n",
    "    \n",
    "    # replace words less than 5 letters with token \"lt_five\"\n",
    "    s = re.sub(r\"\\b\\w{1,4}\\b\", 'lt_five', s)\n",
    "    \n",
    "    return s\n",
    "\n",
    "# using the empty_preprocessor\n",
    "v = CountVectorizer(preprocessor=empty_preprocessor)\n",
    "#     v = CountVectorizer()\n",
    "f = v.fit_transform(train_data)\n",
    "vocab1 = v.vocabulary_\n",
    "d = v.transform(dev_data)\n",
    "lg = LogisticRegression(C=0.18)\n",
    "lg.fit(f, train_labels)\n",
    "pred = lg.predict(d)\n",
    "print '\\nF1-score with empty preprocessor:', \n",
    "print metrics.f1_score(dev_labels, pred, average='weighted')\n",
    "print 'Size of vocab with empty preprocessor:', len(vocab1.keys())\n",
    "\n",
    "# using the better_preprocessor\n",
    "v = CountVectorizer(preprocessor=better_preprocessor)\n",
    "f = v.fit_transform(train_data)\n",
    "vocab2 = v.vocabulary_\n",
    "d = v.transform(dev_data)\n",
    "lg = LogisticRegression(C=0.18)\n",
    "lg.fit(f, train_labels)\n",
    "pred = lg.predict(d)\n",
    "print '\\nF1-score with better preprocessor:', \n",
    "print metrics.f1_score(dev_labels, pred, average='weighted')\n",
    "print 'Size of vocab with better preprocessor:', len(vocab2.keys())\n",
    "\n",
    "### STUDENT END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning the vocabulary using \"l1\" penalty\n",
    "Train a logistic regression model using a \"l1\" penalty. Output the number of learned weights that are not equal to zero. Prune the vocabulary to only those words with at least one non-zero weight when using \"l1\" penalty, and then retrain the model using \"l2\" penalty. Then plot the accuracy of the re-trained model vs. the vocabulary size after pruning unused features while adjusting the regularization parameter `C`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this random seed here to make comparison easier.\n",
    "np.random.seed(0)\n",
    "\n",
    "c_vals = np.arange(0.001,1,0.01)\n",
    "acc = []\n",
    "vocab_size = []\n",
    "for c in c_vals:\n",
    "    # step 1: build the L1 model\n",
    "    v = CountVectorizer()\n",
    "    f = v.fit_transform(train_data)\n",
    "    vocab = v.vocabulary_\n",
    "    lg = LogisticRegression(penalty='l1', tol=0.01, C=c)\n",
    "    lg.fit(f, train_labels)\n",
    "\n",
    "    # step 2: prune the vocab\n",
    "    i,j = np.nonzero(lg.coef_)\n",
    "    vocab_pruned = {k:v for k,v in vocab.items() if v in j}\n",
    "    vocab_size.append(len(vocab_pruned.keys()))\n",
    "\n",
    "    # step 3: train an L2 model with pruned vocab\n",
    "    v = CountVectorizer(vocabulary=vocab_pruned.keys())\n",
    "    f = v.fit_transform(train_data)\n",
    "    p = v.transform(dev_data)\n",
    "    lg = LogisticRegression(penalty='l2', tol=0.01, C=c)\n",
    "    lg.fit(f, train_labels)\n",
    "    pred = lg.predict(p)\n",
    "    acc.append(metrics.accuracy_score(dev_labels, pred))\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(vocab_size, acc, 'o-')\n",
    "plt.xlabel('Size of Pruned Vocabulary')\n",
    "plt.ylabel('Logistic Regression Accuracy using L2 Penalty')\n",
    "\n",
    "start = time.time()\n",
    "print 'runtime:', time.time() - start\n",
    "### STUDENT END ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the TfidfVectorizer to the CountVectorizer\n",
    "Make predictions on the dev data and show the top 3 documents where the ratio R is largest, where R is:\n",
    "> maximum predicted probability / predicted probability of the correct label\n",
    "\n",
    "Determine what mistakes the classifier is making by looking at the top 20 words for each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 3\n",
    "    \n",
    "# build the tfidf transform and fit the training data and the dev\n",
    "# dev data\n",
    "t = TfidfVectorizer()\n",
    "f = t.fit_transform(train_data)\n",
    "d = t.transform(dev_data)\n",
    "\n",
    "# Fit a logistic regression model to the vectorized training data \n",
    "# and predict the probabilities\n",
    "lg = LogisticRegression(C=100)\n",
    "lg.fit(f, train_labels)\n",
    "pred = lg.predict(d)\n",
    "probs = lg.predict_proba(d)\n",
    "\n",
    "# find the maximum probabilities\n",
    "max_probs = np.max(probs, axis=1)\n",
    "\n",
    "# find the proabilities of the correct label\n",
    "correct_probs = np.array([probs[i,dev_labels[i]] for i in range(len(dev_labels))])\n",
    "R = max_probs / correct_probs\n",
    "\n",
    "# display the highest R ratio examples\n",
    "labels = newsgroups_train['target_names']\n",
    "top = np.argsort(R)[::-1][:num_examples]\n",
    "for i in range(num_examples):\n",
    "    print \"\\nSample:\", i\n",
    "    print 'True Label:', labels[dev_labels[top[i]]]\n",
    "    print 'Predicted Label:', labels[pred[top[i]]]\n",
    "    print 'R ratio:', R[top[i]]\n",
    "    print '\\n', dev_data[top[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def top_table(train_data, train_labels, grams=(1,1), num_words=5):\n",
    "    \n",
    "    # get the target labels\n",
    "    labels = newsgroups_train['target_names']\n",
    "    \n",
    "    # vectorize the data and extract the vocabulary\n",
    "    # v = CountVectorizer(ngram_range=grams)\n",
    "    v = TfidfVectorizer(ngram_range=grams)\n",
    "    f = v.fit_transform(train_data)\n",
    "    vocab = v.vocabulary_\n",
    "    \n",
    "    # fit the logisitic regression to extract the coefficients (weights) \n",
    "    # of the feature vectors\n",
    "    lg = LogisticRegression(C=0.18)\n",
    "    lg.fit(f, train_labels)\n",
    "    \n",
    "    # generate the weight map\n",
    "    word_map = {}\n",
    "    words = []\n",
    "    for ind,c in enumerate(lg.coef_):\n",
    "        \n",
    "        # find the top 5 words by weight for each label.\n",
    "        top5 = np.argsort(c)[::-1][:num_words]\n",
    "        words = [(k, v) for k,v in vocab.items() if v in top5]\n",
    "        \n",
    "        # store top 5 for each label in single word map\n",
    "        for w in words:\n",
    "            word_map[w[0]] = w[1]\n",
    "    \n",
    "    # grab the weights at the indices in the word map\n",
    "    weight_arr = np.array([\n",
    "        [lg.coef_[i][v] for v in word_map.values()] \n",
    "        for i in range(len(labels))\n",
    "    ])\n",
    "    \n",
    "    # generate the DataFrame\n",
    "    df = pd.DataFrame(weight_arr.T, columns=labels, index=word_map.keys())\n",
    "    \n",
    "    return df\n",
    "\n",
    "df1 = top_table(train_data, train_labels, num_words=20)\n",
    "print '\\nUnigram features only:\\n'\n",
    "print df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand what mistakes are being made here, I modified the function in porblem 4 to use the TfidfVectorizer instead of the CountVectorizer and I looked at the 20 highest frequency words for each label. In the first example, there are many words present that are only associated with the comp.graphics category, such as \"Apple\", \"Microsoft\", \"ASCII\", \"LaTeX\", etc. Additionally, the word \"file\" appears many times thorughout the document and is strongly associated with the comp.graphics label and very negatively associated with the talk.religion.misc label. So the vectorizer takes these words out of context and the regression just looks at the word frequency and there is not enough intelligence to know that these words that are strongly associated with comp.graphics are used here to promote a religious text.  \n",
    "  \n",
    "The second example does the same thing. The word \"anyone\" is one of the top 20 words in the comp.graphics category and words like \"ftp\" and \"internet\" are only positively associated with the comp.graphics label and since there are very little words in this document, these few words outweigh \"Mormon\".  \n",
    "  \n",
    "The third example has very few words to analyze, but interestingly words such as \"of\", \"the\" and \"children\" were strongly associated with talk.religion.misc and so since there isn't anything stronger to go off of here, those words are probably the key predictors in classifying this document.  \n",
    "\n",
    "Since the TfidfVectorizer just counts the frequency of each word, common short words such as \"is\", \"of\", \"at\", etc. are being given very strong weights in the classification even though they don't actually contain a lot of contextual meaning. One way to improve this would be to change how we define tokens in the vectorizer. If we only looked at words of length 5 letters or more for example, this will put heavier weight on more important words to the context of the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improve the classifier by tokenizing on 5 or more letter words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the original vectorizer with default two-letter token\n",
    "t = TfidfVectorizer()\n",
    "lg = LogisticRegression()\n",
    "f = t.fit_transform(train_data)\n",
    "d = t.transform(dev_data)\n",
    "lg.fit(f, train_labels)\n",
    "pred = lg.predict(d)\n",
    "skore = metrics.f1_score(dev_labels, pred, average='weighted')\n",
    "print 'F1-Score using two or more letters:', skore\n",
    "\n",
    "# testing the vecotrizer with five-letter or more tokens\n",
    "t = TfidfVectorizer(token_pattern=u'(?u)\\\\b\\\\w{5,}\\\\b')\n",
    "lg = LogisticRegression()\n",
    "f = t.fit_transform(train_data)\n",
    "d = t.transform(dev_data)\n",
    "lg.fit(f, train_labels)\n",
    "pred = lg.predict(d)\n",
    "skore = metrics.f1_score(dev_labels, pred, average='weighted')\n",
    "print 'F1-Score using five or more letters:', skore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
